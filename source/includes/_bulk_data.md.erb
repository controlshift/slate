# Bulk Data

To perform advanced reporting or analytics on your ControlShift data, you can mirror all your data into an external database. This can be a helpful tool for answering high-level questions about member engagement.

We provide a set of automated bulk exports and webhooks, along with examples (linked below) on how to use them.

This service was built in a database agnostic way, but it is possible to build a ControlShift &rarr; Amazon Redshift data pipeline using the [ControlShift to Redshift Pipeline](#controlshift-to-redshift-pipeline) technique outlined below.

## Export schedule and webhooks

Every night, we'll export the most up-to-date version of all of your data into a set of CSV files, one for each internal ControlShift table. The [data.full_table_exported](#data-full_table_exported) indicates such an export. These full CSV files should _replace_ the existing data in your mirror database.

Additionally, once a minute, we'll produce CSV files with any new rows that have been _added_ to ControlShift's internal tables. The [data.incremental_table_exported](#data-incremental_table_exported) webhooks indicates a set of these added-rows exports. Note that the incremental exports do _not_ include any updates or deletions of existing rows; you'll have to wait for the nightly export to receive fresh data with updates and deletions included.

<aside class="notice">
Bulk data webhooks should be automatically included when adding a new webhook endpoint. Please contact support to report any issues with bulk data webhook generation. For testing, you can manually trigger these wehbooks by visiting <code>https://&lt;your controlshift instance&gt;/org/settings/integrations/webhook_endpoints</code> and clicking on "Trigger" under "Test Nightly Bulk Data Export Webhook".
</aside>

## Bulk Data Data Schemas

The bulk data webhooks include exports of the following tables:

<% data.export_tables['tables'].each do |tbl_info| %>
* <%= tbl_info['table']['name'] %>
<% end %>

For full information on the schema of each table, use the `/api/bulk_data/schema.json` API endpoint.

## Bulk Data Files

Each table exposed by the bulk data API is made available as a CSV file with the URL to download each file sent via webhook.

We expire access to data 6 hours after it has been generated. This means that if you are building an automated system
to ingest data from this API it must process webhook notifications within 6 hours.

Once webhook data has expired, the download URLs will return a 403 error response.

### Interpreting the share_clicks table

The `share_clicks` table is designed to help you understand in detail how social media sharing influences member actions.
Every time a member clicks on a social sharing link on a petition or event page, it's recorded in the `share_clicks` table,
and we add a unique sharing token to the campaign link they share. When another member follows the shared link and
signs a petition, we record the `referring_share_click_id` for the new signature. Thus it is possible to trace the origin
of a signature to the member who shared a campaign on social media.

#### Auto-generated share clicks

For button clicks on campaign pages, we can create share click records on the fly, using Javascript.
But when members click on social sharing links in emails, or forward campaign emails to their friends, it's impossible
to track that activity directly.

To support the referring-member functionality for emails, we pre-create `share_clicks`
records representing the idea that a user might click on the social sharing links in the Thanks email, or
forward the email to their friends. Then we add the unique sharing tokens to the links in the email, so that any
signatures resulting from those email-based shares can get a `referring_share_click_id`. We do this for both the
Thanks For Signing email, and the Thanks For Creating Petition email.

The pre-created `share_clicks` records have `true` values in the `auto_generated` column, to allow distinguishing them
from records that represent a member definitely clicking on something.

#### Column by column

| Column    | Explanation                                                              |
| --------- | ------------------------------------------------------------------------ |
| id        | Unique identifier for this share click. Not guaranteed to be sequential. |
| page_type | Campaign type (Petition or Event)                                        |
| page_id   | Campaign ID; corresponds to `id` in the `petitions` or `events` table    |
| medium    | What kind of share (e.g. "facebook" or "email")                          |
| member_id | If available, which member did the share click. Corresponds to `id` in the `members` table. Might be null if e.g. a member arrived on a petition page and immediately shared without signing. |
| created_at | When the member clicked on the sharing button. Or for auto-generated share clicks, when the email was sent. |
| updated_at | Usually the same as `created_at`, but would be updated if the record was changed after creation |
| token     | Random unique identifier used to refer to this share click in URLs       |
| auto_generated | Whether this is an auto-generated share click. Boolean.             |

### Interpreting the unsubscribes table

The `unsubscribes` table is used for keeping a record of all member unsubscribes.

There are several kinds of unsubscribes:

* Petition unsubscribe: member is unsubscribing just from a petition they have signed.
* Event unsubscribe: member is unsubscribing just from an event they have RSVPed to.
* Partnership unsubscribe: member is unsubscribing from all actions they have taken in petitions and events from a partner organisation.
* Organisation unsubscribe: member is unsubscribing from all actions they have taken with the organisation.

Depending the kind of unsubscribe the member is taking, the database record will have different values on its attributes:

| Unsubscribe Kind    | Attribute values                                              |
|------------------------|---------------------------------------------------------------|
| Petition unsubscribe | `unsubscribable_type` = `"Signature"`.<br>`unsubscribable_id` corresponds to id in the `signatures` table.<br>`unsubscribe_object` and `unsubscribe_organisation` may be `true` or `false` depending the option the member chooses when unsubscribing. |
| Event unsubscribe | `unsubscribable_type` = `"Attendee"`.<br>`unsubscribable_id` corresponds to id in the `attendees` table.<br>`unsubscribe_object` and `unsubscribe_organisation` may be `true` or `false` depending the option the member chooses when unsubscribing. |
| Partnership unsubscribe | `unsubscribable_type` = `"PartnershipSubscription"`.<br>`unsubscribable_id` corresponds to id in the `partnership_subscriptions` table.<br>`unsubscribe_object` and `unsubscribe_organisation` may be `true` or `false` depending the option the member chooses when unsubscribing. |
| Organisation unsubscribe |  `unsubscribable_type` = `"Organisation"`.<br>`unsubscribe_object` = `false`.<br>`unsubscribe_organisation` = `true`. |

Additionally the following columns apply for all kinds of unsubscribe:

| Column    | Explanation                                                              |
| --------- | ------------------------------------------------------------------------ |
| blast_email_id | Blast email ID; corresponds to `id` in the `blast_emails` table. May be null if unsubscribe was not triggered by the member clicking on a blast email's unsubscribe link (e.g.: when unsubscribing member from the API or from org admin page). |
| email | Member's email address. |
| external_id | If unsubscribe was synced to CRM this column will store the ID on the external system. |
| externally_unsubscribed_at | When the unsubscribe was synced to CRM.  |
| created_at | When the unsubscribe was recorded in the platform. |
| updated_at | Same as `created_at` as unsubscribe records cannot be updated after creation. |

## ControlShift to Redshift Pipeline

Setting up an Amazon Redshift integration is a great way to learn more about the actions your members are taking or perform
sophisticated analytics, but it is an advanced topic that requires knowledge of Amazon Web Services, SQL, and Terraform.



Following these instructions we'll use a custom [AWS Lambda](http://docs.aws.amazon.com/lambda/latest/dg/welcome.html)
to receive data from ControlShift's bulk data API webhooks and two different mechanisms to load our data into Redshift:

* Amazon's [Lambda Redshift Loader](https://github.com/awslabs/aws-lambda-redshift-loader), for processing incremental exports and full table exports for small tables.
* [AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html), for processing full table exports for large tables (i.e.: `signatures`).

The final result will be a replica of the tables that back your instance of ControlShift that are refreshed in
full each night and appended with inserts on a per-minute basis.

ControlShift has authored a Terraform module that makes the entire setup process straightforward and allows for updates
to the infrastructure over time.

### Example Flow

#### ControlShift Redshift Flow For Small Data Exports

![ControlShift Redshift Flow With Lambdas Chart](/images/ControlShift_Redshift_flow_with_Lambdas_transparent.svg)

#### ControlShift Redshift Flow For Large Data Exports

![ControlShift Redshift Flow With Glue Chart](/images/ControlShift_Redshift_flow_with_Glue_transparent.svg)


### Redshift Pipeline Setup

If you are not yet using Redshift or Terraform, we've written [an example Terraform plan](https://github.com/controlshift/bulk-data-example)
that should create a working integration, a new Redshift instance and appropriate permissions from an otherwise empty AWS Account.

If you are already using Terraform or Redshift it is probably best to either fork this example or
 [use the module we provide directly in your own TF plan](https://registry.terraform.io/modules/controlshift/controlshift-redshift-sync/aws/)

Applying the Terraform plan in your AWS account will create all of the resources necessary and output the Webhook URL that you'll need to configure within
your ControlShift platform settings.

#### Prepare Redshift Schema

We need to prep tables in your Redshift Data Warehouse to receive our ControlShift Data.  We'll use `psql` for this.

1. Install `psql` if you don't have it already.
2. Download the current SQL DDL for ControlShift's tables <a href="/data/tables.sql" target="_blank">tables.sql</a>
or [generate an up to date one using this script.](https://github.com/controlshift/bulk-data-example/blob/master/create_tables.rb) that uses our Schema API.
3. Finally, you'll need to load that set of tables into your Redshift instance with the following command:

```psql -h <cluster_endpoint> -U <database_user> -d <database_name> -p <cluster_port> -f petitions_schema.sql```

### Configure ControlShift's Webhook

Finally, you'll need to log into your admin panel.  Settings > Integrations > Webhooks.  Then add the bulk data webhook pointing at your Lambda endpoint output by the Terraform plan.

### More information.

More information is available on the READMEs for the various projects that this integration is composed from. We can also provide assistance
for customers configuring these tools via support@controlshiftlabs.com
