# Bulk Data

ControlShift's Bulk Data Webhooks make it easy to pull your data into external services.

## Webhooks

We provides two special bulk data webhooks to help you keep an external reporting or analytics database server up to date with information from ControlShift's internal tables. The [data.full_table_exported](#data-full_table_exported) and [data.incremental_table_exported](#data-incremental_table_exported) webhooks can be consumed to keep an external database mirror containing ControlShift data up to date. This service was built in a database agnostic way, but it should be possible to build a ControlShift -> Amazon Redshift data pipeline using the [ControlShift to Redshift Pipeline](#controlshift-to-redshift-pipeline) technique outlines below.

<aside class="notice">
Bulk data webhooks should be automatically included when adding a new webhook endpoint. Please contact support to report any issues with bulk data webhook generation. For testing, you can manually trigger these wehbooks by visiting <code>https://&lt;your controlshift instance&gt;/org/settings/integrations/webhook_endpoints</code> and clicking on "Trigger" under "Test Nightly Bulk Data Export Webhook".
</aside>

## Bulk Data Data Schemas

The bulk data webhooks include exports of the following tables:

<% data.export_tables['tables'].each do |tbl_info| %>
* <%= tbl_info['table']['name'] %>
<% end %>

For full information on the schema of each table, use the `/api/bulk_data/schema.json` API endpoint.

## Bulk Data Files

Each table exposed by the bulk data API is made available as a CSV file with the URL to download each file sent via webhook.

We expire access to data 24 hours after it has been generated. This means that if you are building an automated system
to ingest data from this API it must process webhook notifications within 24 hours.

Once webhook data has expired, the download URLs will return a 403 error response.

### Interpreting the share_clicks table

The `share_clicks` table is designed to help you understand in detail how social media sharing influences member actions.
Every time a member clicks on a social sharing link on a petition or event page, it's recorded in the `share_clicks` table,
and we add a unique sharing token to the campaign link they share. When another member follows the shared link and
signs a petition, we record the `referring_share_click_id` for the new signature. Thus it is possible to trace the origin
of a signature to the member who shared a campaign on social media.

#### Auto-generated share clicks

For button clicks on campaign pages, we can create share click records on the fly, using Javascript.
But when members click on social sharing links in emails, or forward campaign emails to their friends, it's impossible
to track that activity directly.

To support the referring-member functionality for emails, we pre-create `share_clicks`
records representing the idea that a user might click on the social sharing links in the Thanks email, or
forward the email to their friends. Then we add the unique sharing tokens to the links in the email, so that any
signatures resulting from those email-based shares can get a `referring_share_click_id`. We do this for both the
Thanks For Signing email, and the Thanks For Creating Petition email.

The pre-created `share_clicks` records have `true` values in the `auto_generated` column, to allow distinguishing them
from records that represent a member definitely clicking on something.

#### Column by column

| Column    | Explanation                                                              |
| --------- | ------------------------------------------------------------------------ |
| id        | Unique identifier for this share click. Not guaranteed to be sequential. |
| page_type | Campaign type (Petition or Event)                                        |
| page_id   | Campaign ID; corresponds to `id` in the `petitions` or `events` table    |
| medium    | What kind of share (e.g. "facebook" or "email")                          |
| member_id | If available, which member did the share click. Corresponds to `id` in the `members` table. Might be null if e.g. a member arrived on a petition page and immediately shared without signing. |
| created_at | When the member clicked on the sharing button. Or for auto-generated share clicks, when the email was sent. |
| updated_at | Usually the same as `created_at`, but would be updated if the record was changed after creation |
| token     | Random unique identifier used to refer to this share click in URLs       |
| auto_generated | Whether this is an auto-generated share click. Boolean.             |

## ControlShift to Redshift Pipeline

Setting up an Amazon Redshift integration is a great way to learn more about the actions your members are taking or perform
sophisticated analytics, but it is an advanced topic that requires knowledge of Amazon Web Services, SQL, and Terraform.



Following these instructions we'll use a custom [AWS Lambda](http://docs.aws.amazon.com/lambda/latest/dg/welcome.html)
to receive data from ControlShift's bulk data API webhooks and two different mechanisms to load our data into Redshift:

* Amazon's [Lambda Redshift Loader](https://github.com/awslabs/aws-lambda-redshift-loader), for processing incremental exports and full table exports for small tables.
* [AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html), for processing full table exports for large tables (i.e.: `signatures`).

The final result will be a replica of the tables that back your instance of ControlShift that are refreshed in
full each night and appended with inserts on a per-minute basis.

ControlShift has authored a Terraform module that makes the entire setup process straightforward and allows for updates
to the infrastructure over time.

### Example Flow

#### ControlShift Redshift Flow For Small Data Exports

![ControlShift Redshift Flow With Lambdas Chart](/images/ControlShift_Redshift_flow_with_Lambdas_transparent.svg)

#### ControlShift Redshift Flow For Large Data Exports

![ControlShift Redshift Flow With Glue Chart](/images/ControlShift_Redshift_flow_with_Glue_transparent.svg)


### Redshift Pipeline Setup

If you are not yet using Redshift or Terraform, we've written [an example Terraform plan](https://github.com/controlshift/bulk-data-example)
that should create a working integration, a new Redshift instance and appropriate permissions from an otherwise empty AWS Account.

If you are already using Terraform or Redshift it is probably best to either fork this example or
 [use the module we provide directly in your own TF plan](https://registry.terraform.io/modules/controlshift/controlshift-redshift-sync/aws/)

Applying the Terraform plan in your AWS account will create all of the resources necessary and output the Webhook URL that you'll need to configure within
your ControlShift platform settings.

#### Prepare Redshift Schema

We need to prep tables in your Redshift Data Warehouse to receive our ControlShift Data.  We'll use `psql` for this.

1. Install `psql` if you don't have it already.
2. Download the current SQL DDL for ControlShift's tables <a href="/data/tables.sql" target="_blank">tables.sql</a>
or [generate an up to date one using this script.](https://github.com/controlshift/bulk-data-example/blob/master/create_tables.rb) that uses our Schema API.
3. Finally, you'll need to load that set of tables into your Redshift instance with the following command:

```psql -h <cluster_endpoint> -U <database_user> -d <database_name> -p <cluster_port> -f petitions_schema.sql```

### Configure ControlShift's Webhook

Finally, you'll need to log into your admin panel.  Settings > Integrations > Webhooks.  Then add the bulk data webhook pointing at your Lambda endpoint output by the Terraform plan.

### More information.

More information is available on the READMEs for the various projects that this integration is composed from. We can also provide assistance
for customers configuring these tools via support@controlshiftlabs.com
